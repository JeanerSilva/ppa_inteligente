import os
import time
import logging
import streamlit as st
from settings import RETRIEVER_TOP_K, EMBEDDING_OPTIONS

from PIL import Image
from rag.vectorstore import load_vectorstore, create_vectorstore
from rag.qa_chain import build_qa_chain
from rag.utils import save_uploaded_files, load_indexed_files
from rag.llm_loader import load_llm
from rag.chat_history import generate_session_id, save_chat
from rag.prompt import get_saved_prompts, save_prompt, get_prompt


# Configura√ß√£o de logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
    handlers=[
        logging.FileHandler("ppa.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)

st.set_page_config(page_title="PPA Inteligente", page_icon="üßê")
logging.info("Aplicativo iniciado.")

img = Image.open("ppa.png")
img_resized = img.resize((150, 75))
st.image(img_resized)
st.title("üßê Pergunte ao PPA")

# Estado inicial
if "indexed_files" not in st.session_state:
    st.session_state["indexed_files"] = load_indexed_files()
    logging.info("Arquivos indexados carregados do estado inicial.")

if "chat_history" not in st.session_state:
    st.session_state.chat_history = []

if "chat_session_id" not in st.session_state:
    st.session_state.chat_session_id = generate_session_id()
    logging.info(f"Sess√£o iniciada: {st.session_state.chat_session_id}")

# Prompt personalizado
st.subheader("üõ†Ô∏è Prompts personalizados")
prompt_text = get_prompt("teste")


prompts = get_saved_prompts()
prompt_names = list(prompts.keys()) or ["default"]

prompt_selecionado = st.selectbox("Escolha um prompt para editar ou criar:", prompt_names + ["<novo>"], key="prompt_selector")

if prompt_selecionado == "<novo>":
    novo_nome = st.text_input("Nome do novo prompt:", key="novo_nome_prompt")
    prompt_conteudo = ""
else:
    novo_nome = prompt_selecionado
    prompt_conteudo = prompts.get(prompt_selecionado, "")

edited_prompt = st.text_area(
    "Conte√∫do do prompt (use {context} e {question}):",
    value=prompt_conteudo,
    height=400,
    key="prompt_editor"
)

if st.button("üíæ Salvar prompt"):
    if novo_nome.strip() == "":
        st.warning("Nome do prompt n√£o pode estar vazio.")
    else:
        save_prompt(novo_nome.strip(), edited_prompt)
        st.session_state["prompt_template"] = edited_prompt
        st.session_state["prompt_name"] = novo_nome
        st.success(f"Prompt '{novo_nome}' salvo com sucesso!")
        st.rerun()

# Atualiza para uso no QA
st.session_state["prompt_template"] = edited_prompt


# Sidebar: Configura√ß√µes
st.sidebar.markdown("‚öôÔ∏è **Configura√ß√µes**")
st.session_state["retriever_k"] = st.sidebar.number_input(
    label="N√∫mero de trechos a considerar (k)",
    min_value=1,
    max_value=20,
    value=st.session_state.get("retriever_k", RETRIEVER_TOP_K),
    step=1
)

# Sidebar: LLM
st.sidebar.markdown("üß† **Modelo de linguagem**")
modelo_llm = st.sidebar.radio(
    "Modo de execu√ß√£o:",
    ["Ollama (servidor)", "OpenAI (API)"]
)
st.session_state["modelo_llm"] = modelo_llm
logging.info(f"Modo de execu√ß√£o selecionado: {modelo_llm}")

# Sidebar: Embedding
st.sidebar.markdown("üß¨ **Modelo de embedding**")
embed_model_label = st.sidebar.selectbox("Escolha o modelo:", list(EMBEDDING_OPTIONS.keys()))
embed_model_name = EMBEDDING_OPTIONS[embed_model_label]
st.session_state["embedding_model"] = embed_model_name
logging.info(f"Modelo de embedding selecionado: {embed_model_name}")

# Sidebar: Reindexar
if st.sidebar.button("üîÅ Reindexar agora"):
    logging.info("Iniciando reindexa√ß√£o manual...")
    try:
        with st.spinner("üîÑ Indexando documentos e criando vetor..."):
            db, metrics = create_vectorstore(embed_model_name)
            if db is None:
                st.error("‚ùå A indexa√ß√£o falhou. Nenhum vetor foi criado.")
            else:
                st.success("‚úÖ Vetor criado com sucesso!")
                st.session_state["index_metrics"] = metrics
    except Exception as e:
        st.error(f"‚ùå Erro ao criar o vetor: {e}")
        logging.exception("Erro durante cria√ß√£o da base vetorial.")

# Painel de m√©tricas de indexa√ß√£o
if "index_metrics" in st.session_state:
    st.subheader("üìä M√©tricas da √∫ltima indexa√ß√£o")

    m = st.session_state["index_metrics"]
    col1, col2, col3 = st.columns(3)
    col1.metric("‚è±Ô∏è Tempo (s)", f"{m['tempo_total']:.2f}")
    col2.metric("üìÑ Arquivos", m["arquivos_processados"])
    col3.metric("üîé Chunks", m["chunks_gerados"])

    st.markdown(f"‚úÖ Sucesso: `{m['sucesso']}` &nbsp;&nbsp;&nbsp;&nbsp; ‚ùå Falha: `{m['falha']}`")

    with st.expander("üìÅ Arquivos processados"):
        for f in m["arquivos"]:
            st.markdown(f"- `{f}`")

# Sidebar: arquivos indexados
indexed_files = st.session_state.get("indexed_files", [])
if indexed_files:
    st.sidebar.markdown("üìÇ **Arquivos indexados:**", unsafe_allow_html=True)
    st.sidebar.markdown(
        "<ul style='padding-left:1.2em;'>"
        + "".join(f"<li style='font-size:0.8em;'>{f}</li>" for f in indexed_files)
        + "</ul>", unsafe_allow_html=True
    )
else:
    st.sidebar.info("Nenhum arquivo indexado.")

# Sidebar: Upload
st.sidebar.header("üìÑ Enviar documentos")
uploaded_files = st.sidebar.file_uploader(
    "Arquivos: .pdf, .txt, .docx, .xlsx, .html",
    type=["pdf", "txt", "docx", "xlsx", "html"],
    accept_multiple_files=True,
)
if uploaded_files:
    save_uploaded_files(uploaded_files)
    st.sidebar.success("‚úÖ Arquivos enviados com sucesso.")
    logging.info(f"{len(uploaded_files)} arquivos enviados e salvos.")

# Carregar index e LLM
vectorstore = load_vectorstore(embed_model_name)
if not vectorstore:
    st.warning("‚ö†Ô∏è Nenhum √≠ndice encontrado para esse modelo. Reindexe primeiro.")
    logging.warning("Nenhum √≠ndice encontrado. Solicita√ß√£o de reindexa√ß√£o.")
    st.stop()

llm = load_llm(modelo_llm)
#qa_chain = build_qa_chain(vectorstore, llm, st.session_state["prompt_template"])
qa_chain = build_qa_chain(vectorstore, llm, st.session_state.get("prompt_name", "teste"))

if not qa_chain:
    st.warning("‚ö†Ô∏è A chain n√£o est√° carregada.")
    logging.error("Falha ao carregar a chain.")
    st.stop()

# Formul√°rio de pergunta
with st.form("chat-form", clear_on_submit=True):
    user_input = st.text_input("Digite sua pergunta:", value="")
    submitted = st.form_submit_button("Enviar")

if submitted and user_input:
    query = f"query: {user_input}"
    logging.info(f"Pergunta enviada: {user_input}")
    start = time.time()
    result = qa_chain.invoke({"query": query})
    elapsed = time.time() - start

    resposta = result["result"]
    fontes = result["source_documents"]

    st.session_state.chat_history.append(("user", user_input))
    st.session_state.chat_history.append(("bot", resposta))
    st.session_state.last_contexts = fontes

    # Metadados da sess√£o
    chat_metadata = {
        "modelo_llm": modelo_llm,
        "modelo_embedding": embed_model_name,
        "retriever_k": st.session_state["retriever_k"],
        "timestamp": time.strftime("%Y-%m-%d %H:%M:%S"),
    }

    save_chat(
        st.session_state.chat_session_id,
        st.session_state.chat_history,
        metadata=chat_metadata
    )

    logging.info(f"Resposta gerada em {elapsed:.2f} segundos.")
    st.sidebar.success(f"‚è±Ô∏è Resposta em {elapsed:.2f} segundos")

    with st.expander("üîå Depura√ß√£o: Chunks retornados pelo retriever"):
        for doc in fontes:
            st.markdown(doc.page_content)

# Exibi√ß√£o estilo chat
for role, msg in st.session_state.chat_history:
    with st.chat_message("user" if role == "user" else "assistant"):
        st.markdown(msg)

# Fontes da resposta
if "last_contexts" in st.session_state:
    with st.expander("üìö Trechos usados na resposta"):
        for doc in st.session_state.last_contexts:
            source = doc.metadata.get("source", "desconhecido")
            nome = os.path.basename(source)
            tipo = os.path.splitext(nome)[1].replace(".", "").upper()
            st.markdown(f"**Fonte:** `{nome}` ({tipo})")
            st.markdown(doc.page_content.strip())
            st.markdown("---")

# Limpar conversa
if st.button("üßπ Limpar conversa"):
    st.session_state.chat_history = []
    st.session_state.last_contexts = []
    logging.info("Conversa limpa pelo usu√°rio.")
    st.rerun()

# Download da resposta
if st.session_state.chat_history:
    for role, msg in reversed(st.session_state.chat_history):
        if role == "bot":
            st.download_button("üìÖ Baixar √∫ltima resposta", msg, file_name="resposta.txt")
            break
